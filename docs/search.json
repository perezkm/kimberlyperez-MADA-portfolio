[
  {
<<<<<<< Updated upstream
<<<<<<< Updated upstream
    "objectID": "visualization_exercise.html",
    "href": "visualization_exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "The Data\nThe graph I attempted to reproduce below was retrieved from a FiveThirtyEight article “The National Parks Have Never Been More Popular”, by Andrew Flowers, that visualizes the rise in popularity of the US’s national parks throughout the years. The graph was created from annual visitor counts that date back to 1904. Based on these visitor counts, Flowers then ranked the national parks (e.g., park with most visitor count ranked highest). The National Park Services (NPS) have several detailed data sets that can be found here. The data set I utilized for this visualization exercise can be found on the aforementioned NPS page under the “Annual Visitation and Record Year by Park (1904-Last Calendar Year)”. You will be prompted to a page with three drop down menus. For”Region(s)” check “(Select All)”. Next, for “Park(s)” check “(Select All)”. Finally, for “Park Type”, scroll and check “National Park”. This will then return the data needed to replicate the graph below.\n\n\n\nLoading R Libraries\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.5.0 \n✔ tidyr   1.3.0      ✔ forcats 0.5.2 \n✔ purrr   1.0.1      \n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(fivethirtyeight)\n\nSome larger datasets need to be installed separately, like senators and\nhouse_district_forecast. To install these, we recommend you install the\nfivethirtyeightdata package by running:\ninstall.packages('fivethirtyeightdata', repos =\n'https://fivethirtyeightdata.github.io/drat/', type = 'source')\n\nlibrary(ggthemes)\nlibrary(dplyr)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tibble)\n\n\n\nData Loading, Exploring, and Cleaning\n\norig_nps<-read_csv(\"data/NPS Visitation Data.csv\")\n\nNew names:\nRows: 67 Columns: 122\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): ...1, ...2, ...3 num (118): ...4, ...5, ...7, ...8, ...9, ...10, ...11,\n...12, ...13, ...14, ... lgl (1): ...6\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n• `` -> `...2`\n• `` -> `...3`\n• `` -> `...4`\n• `` -> `...5`\n• `` -> `...6`\n• `` -> `...7`\n• `` -> `...8`\n• `` -> `...9`\n• `` -> `...10`\n• `` -> `...11`\n• `` -> `...12`\n• `` -> `...13`\n• `` -> `...14`\n• `` -> `...15`\n• `` -> `...16`\n• `` -> `...17`\n• `` -> `...18`\n• `` -> `...19`\n• `` -> `...20`\n• `` -> `...21`\n• `` -> `...22`\n• `` -> `...23`\n• `` -> `...24`\n• `` -> `...25`\n• `` -> `...26`\n• `` -> `...27`\n• `` -> `...28`\n• `` -> `...29`\n• `` -> `...30`\n• `` -> `...31`\n• `` -> `...32`\n• `` -> `...33`\n• `` -> `...34`\n• `` -> `...35`\n• `` -> `...36`\n• `` -> `...37`\n• `` -> `...38`\n• `` -> `...39`\n• `` -> `...40`\n• `` -> `...41`\n• `` -> `...42`\n• `` -> `...43`\n• `` -> `...44`\n• `` -> `...45`\n• `` -> `...46`\n• `` -> `...47`\n• `` -> `...48`\n• `` -> `...49`\n• `` -> `...50`\n• `` -> `...51`\n• `` -> `...52`\n• `` -> `...53`\n• `` -> `...54`\n• `` -> `...55`\n• `` -> `...56`\n• `` -> `...57`\n• `` -> `...58`\n• `` -> `...59`\n• `` -> `...60`\n• `` -> `...61`\n• `` -> `...62`\n• `` -> `...63`\n• `` -> `...64`\n• `` -> `...65`\n• `` -> `...66`\n• `` -> `...67`\n• `` -> `...68`\n• `` -> `...69`\n• `` -> `...70`\n• `` -> `...71`\n• `` -> `...72`\n• `` -> `...73`\n• `` -> `...74`\n• `` -> `...75`\n• `` -> `...76`\n• `` -> `...77`\n• `` -> `...78`\n• `` -> `...79`\n• `` -> `...80`\n• `` -> `...81`\n• `` -> `...82`\n• `` -> `...83`\n• `` -> `...84`\n• `` -> `...85`\n• `` -> `...86`\n• `` -> `...87`\n• `` -> `...88`\n• `` -> `...89`\n• `` -> `...90`\n• `` -> `...91`\n• `` -> `...92`\n• `` -> `...93`\n• `` -> `...94`\n• `` -> `...95`\n• `` -> `...96`\n• `` -> `...97`\n• `` -> `...98`\n• `` -> `...99`\n• `` -> `...100`\n• `` -> `...101`\n• `` -> `...102`\n• `` -> `...103`\n• `` -> `...104`\n• `` -> `...105`\n• `` -> `...106`\n• `` -> `...107`\n• `` -> `...108`\n• `` -> `...109`\n• `` -> `...110`\n• `` -> `...111`\n• `` -> `...112`\n• `` -> `...113`\n• `` -> `...114`\n• `` -> `...115`\n• `` -> `...116`\n• `` -> `...117`\n• `` -> `...118`\n• `` -> `...119`\n• `` -> `...120`\n• `` -> `...121`\n• `` -> `...122`\n\nstr(orig_nps) #Exploring the NPS data a bit with these commands\n\nspc_tbl_ [67 × 122] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ ...1  : chr [1:67] \"Annual Visitation and Record Year by Park (1904 - Last Calendar Year)\" NA \"Region\" \"Alaska Region\" ...\n $ ...2  : chr [1:67] NA NA \"Park Name\" \"Denali NP & PRES\" ...\n $ ...3  : chr [1:67] NA NA \"Park Type\" \"National Park\" ...\n $ ...4  : num [1:67] NA NA 1904 NA NA ...\n $ ...5  : num [1:67] NA NA 1905 NA NA ...\n $ ...6  : logi [1:67] NA NA NA NA NA NA ...\n $ ...7  : num [1:67] NA NA 1906 NA NA ...\n $ ...8  : num [1:67] NA NA 1907 NA NA ...\n $ ...9  : num [1:67] NA NA 1908 NA NA ...\n $ ...10 : num [1:67] NA NA 1909 NA NA ...\n $ ...11 : num [1:67] NA NA 1910 NA NA NA NA NA NA NA ...\n $ ...12 : num [1:67] NA NA 1911 NA NA ...\n $ ...13 : num [1:67] NA NA 1912 NA NA ...\n $ ...14 : num [1:67] NA NA 1913 NA NA ...\n $ ...15 : num [1:67] NA NA 1914 NA NA ...\n $ ...16 : num [1:67] NA NA 1915 NA NA ...\n $ ...17 : num [1:67] NA NA 1916 NA NA ...\n $ ...18 : num [1:67] NA NA 1917 NA NA ...\n $ ...19 : num [1:67] NA NA 1918 NA NA ...\n $ ...20 : num [1:67] NA NA 1919 NA NA ...\n $ ...21 : num [1:67] NA NA 1920 NA NA NA NA NA NA NA ...\n $ ...22 : num [1:67] NA NA 1921 NA NA ...\n $ ...23 : num [1:67] NA NA 1922 7 NA ...\n $ ...24 : num [1:67] NA NA 1923 34 NA ...\n $ ...25 : num [1:67] NA NA 1924 62 NA ...\n $ ...26 : num [1:67] NA NA 1925 206 NA ...\n $ ...27 : num [1:67] NA NA 1926 533 NA ...\n $ ...28 : num [1:67] NA NA 1927 651 NA ...\n $ ...29 : num [1:67] NA NA 1928 802 NA ...\n $ ...30 : num [1:67] NA NA 1929 1038 NA ...\n $ ...31 : num [1:67] NA NA 1930 951 NA NA NA NA NA NA ...\n $ ...32 : num [1:67] NA NA 1931 771 NA ...\n $ ...33 : num [1:67] NA NA 1932 357 NA ...\n $ ...34 : num [1:67] NA NA 1933 386 NA ...\n $ ...35 : num [1:67] NA NA 1934 628 NA ...\n $ ...36 : num [1:67] NA NA 1935 877 NA ...\n $ ...37 : num [1:67] NA NA 1936 1073 NA ...\n $ ...38 : num [1:67] NA NA 1937 1378 NA ...\n $ ...39 : num [1:67] NA NA 1938 1487 NA ...\n $ ...40 : num [1:67] NA NA 1939 2262 NA ...\n $ ...41 : num [1:67] NA NA 1940 1201 NA ...\n $ ...42 : num [1:67] NA NA 1941 1688 NA ...\n $ ...43 : num [1:67] NA NA 1942 5 NA ...\n $ ...44 : num [1:67] NA NA 1943 12 NA ...\n $ ...45 : num [1:67] NA NA 1944 0 NA ...\n $ ...46 : num [1:67] NA NA 1945 19 NA ...\n $ ...47 : num [1:67] NA NA 1946 1134 NA ...\n $ ...48 : num [1:67] NA NA 1947 3466 NA ...\n $ ...49 : num [1:67] NA NA 1948 4512 NA ...\n $ ...50 : num [1:67] NA NA 1949 4831 NA ...\n $ ...51 : num [1:67] NA NA 1950 6672 NA ...\n $ ...52 : num [1:67] NA NA 1951 7807 NA ...\n $ ...53 : num [1:67] NA NA 1952 7310 NA ...\n $ ...54 : num [1:67] NA NA 1953 6839 NA ...\n $ ...55 : num [1:67] NA NA 1954 5000 NA ...\n $ ...56 : num [1:67] NA NA 1955 3400 NA ...\n $ ...57 : num [1:67] NA NA 1956 5200 NA ...\n $ ...58 : num [1:67] NA NA 1957 10700 NA ...\n $ ...59 : num [1:67] NA NA 1958 25900 NA ...\n $ ...60 : num [1:67] NA NA 1959 25800 NA ...\n $ ...61 : num [1:67] NA NA 1960 22500 NA 900 600 NA NA NA ...\n $ ...62 : num [1:67] NA NA 1961 18300 NA ...\n $ ...63 : num [1:67] NA NA 1962 16600 NA ...\n $ ...64 : num [1:67] NA NA 1963 18400 NA ...\n $ ...65 : num [1:67] NA NA 1964 19200 NA ...\n $ ...66 : num [1:67] NA NA 1965 21400 NA ...\n $ ...67 : num [1:67] NA NA 1966 31300 NA ...\n $ ...68 : num [1:67] NA NA 1967 39800 NA ...\n $ ...69 : num [1:67] NA NA 1968 33300 NA ...\n $ ...70 : num [1:67] NA NA 1969 45400 NA ...\n $ ...71 : num [1:67] NA NA 1970 46000 NA 29700 11800 NA NA NA ...\n $ ...72 : num [1:67] NA NA 1971 44500 NA ...\n $ ...73 : num [1:67] NA NA 1972 88625 NA ...\n $ ...74 : num [1:67] NA NA 1973 137300 NA ...\n $ ...75 : num [1:67] NA NA 1974 161400 NA ...\n $ ...76 : num [1:67] NA NA 1975 160600 NA ...\n $ ...77 : num [1:67] NA NA 1976 157600 NA ...\n $ ...78 : num [1:67] NA NA 1977 183200 NA ...\n $ ...79 : num [1:67] NA NA 1978 223000 NA ...\n $ ...80 : num [1:67] NA NA 1979 251105 NA ...\n $ ...81 : num [1:67] NA NA 1980 216361 NA ...\n $ ...82 : num [1:67] NA NA 1981 256593 NA ...\n $ ...83 : num [1:67] NA NA 1982 321868 1381 ...\n $ ...84 : num [1:67] NA NA 1983 346082 2138 ...\n $ ...85 : num [1:67] NA NA 1984 395099 2440 ...\n $ ...86 : num [1:67] NA NA 1985 436545 1381 ...\n $ ...87 : num [1:67] NA NA 1986 529749 2801 ...\n $ ...88 : num [1:67] NA NA 1987 575013 1060 ...\n $ ...89 : num [1:67] NA NA 1988 592431 1258 ...\n $ ...90 : num [1:67] NA NA 1989 543640 822 ...\n $ ...91 : num [1:67] NA NA 1990 546693 1010 ...\n $ ...92 : num [1:67] NA NA 1991 558870 1154 ...\n $ ...93 : num [1:67] NA NA 1992 503674 2116 ...\n $ ...94 : num [1:67] NA NA 1993 505565 2245 ...\n $ ...95 : num [1:67] NA NA 1994 490311 1726 ...\n $ ...96 : num [1:67] NA NA 1995 543309 7074 ...\n $ ...97 : num [1:67] NA NA 1996 341385 6448 ...\n $ ...98 : num [1:67] NA NA 1997 354278 6949 ...\n $ ...99 : num [1:67] NA NA 1998 372519 8266 ...\n  [list output truncated]\n - attr(*, \"spec\")=\n  .. cols(\n  ..   ...1 = col_character(),\n  ..   ...2 = col_character(),\n  ..   ...3 = col_character(),\n  ..   ...4 = col_number(),\n  ..   ...5 = col_number(),\n  ..   ...6 = col_logical(),\n  ..   ...7 = col_number(),\n  ..   ...8 = col_number(),\n  ..   ...9 = col_number(),\n  ..   ...10 = col_number(),\n  ..   ...11 = col_number(),\n  ..   ...12 = col_number(),\n  ..   ...13 = col_number(),\n  ..   ...14 = col_number(),\n  ..   ...15 = col_number(),\n  ..   ...16 = col_number(),\n  ..   ...17 = col_number(),\n  ..   ...18 = col_number(),\n  ..   ...19 = col_number(),\n  ..   ...20 = col_number(),\n  ..   ...21 = col_number(),\n  ..   ...22 = col_number(),\n  ..   ...23 = col_number(),\n  ..   ...24 = col_number(),\n  ..   ...25 = col_number(),\n  ..   ...26 = col_number(),\n  ..   ...27 = col_number(),\n  ..   ...28 = col_number(),\n  ..   ...29 = col_number(),\n  ..   ...30 = col_number(),\n  ..   ...31 = col_number(),\n  ..   ...32 = col_number(),\n  ..   ...33 = col_number(),\n  ..   ...34 = col_number(),\n  ..   ...35 = col_number(),\n  ..   ...36 = col_number(),\n  ..   ...37 = col_number(),\n  ..   ...38 = col_number(),\n  ..   ...39 = col_number(),\n  ..   ...40 = col_number(),\n  ..   ...41 = col_number(),\n  ..   ...42 = col_number(),\n  ..   ...43 = col_number(),\n  ..   ...44 = col_number(),\n  ..   ...45 = col_number(),\n  ..   ...46 = col_number(),\n  ..   ...47 = col_number(),\n  ..   ...48 = col_number(),\n  ..   ...49 = col_number(),\n  ..   ...50 = col_number(),\n  ..   ...51 = col_number(),\n  ..   ...52 = col_number(),\n  ..   ...53 = col_number(),\n  ..   ...54 = col_number(),\n  ..   ...55 = col_number(),\n  ..   ...56 = col_number(),\n  ..   ...57 = col_number(),\n  ..   ...58 = col_number(),\n  ..   ...59 = col_number(),\n  ..   ...60 = col_number(),\n  ..   ...61 = col_number(),\n  ..   ...62 = col_number(),\n  ..   ...63 = col_number(),\n  ..   ...64 = col_number(),\n  ..   ...65 = col_number(),\n  ..   ...66 = col_number(),\n  ..   ...67 = col_number(),\n  ..   ...68 = col_number(),\n  ..   ...69 = col_number(),\n  ..   ...70 = col_number(),\n  ..   ...71 = col_number(),\n  ..   ...72 = col_number(),\n  ..   ...73 = col_number(),\n  ..   ...74 = col_number(),\n  ..   ...75 = col_number(),\n  ..   ...76 = col_number(),\n  ..   ...77 = col_number(),\n  ..   ...78 = col_number(),\n  ..   ...79 = col_number(),\n  ..   ...80 = col_number(),\n  ..   ...81 = col_number(),\n  ..   ...82 = col_number(),\n  ..   ...83 = col_number(),\n  ..   ...84 = col_number(),\n  ..   ...85 = col_number(),\n  ..   ...86 = col_number(),\n  ..   ...87 = col_number(),\n  ..   ...88 = col_number(),\n  ..   ...89 = col_number(),\n  ..   ...90 = col_number(),\n  ..   ...91 = col_number(),\n  ..   ...92 = col_number(),\n  ..   ...93 = col_number(),\n  ..   ...94 = col_number(),\n  ..   ...95 = col_number(),\n  ..   ...96 = col_number(),\n  ..   ...97 = col_number(),\n  ..   ...98 = col_number(),\n  ..   ...99 = col_number(),\n  ..   ...100 = col_number(),\n  ..   ...101 = col_number(),\n  ..   ...102 = col_number(),\n  ..   ...103 = col_number(),\n  ..   ...104 = col_number(),\n  ..   ...105 = col_number(),\n  ..   ...106 = col_number(),\n  ..   ...107 = col_number(),\n  ..   ...108 = col_number(),\n  ..   ...109 = col_number(),\n  ..   ...110 = col_number(),\n  ..   ...111 = col_number(),\n  ..   ...112 = col_number(),\n  ..   ...113 = col_number(),\n  ..   ...114 = col_number(),\n  ..   ...115 = col_number(),\n  ..   ...116 = col_number(),\n  ..   ...117 = col_number(),\n  ..   ...118 = col_number(),\n  ..   ...119 = col_number(),\n  ..   ...120 = col_number(),\n  ..   ...121 = col_number(),\n  ..   ...122 = col_number()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\nsummary(orig_nps)\n\n     ...1               ...2               ...3                ...4       \n Length:67          Length:67          Length:67          Min.   :   563  \n Class :character   Class :character   Class :character   1st Qu.:  1250  \n Mode  :character   Mode  :character   Mode  :character   Median :  1904  \n                                                          Mean   : 17513  \n                                                          3rd Qu.:  8314  \n                                                          Max.   :101000  \n                                                          NA's   :60      \n      ...5          ...6              ...7            ...8      \n Min.   :   928   Mode:logical   Min.   :  700   Min.   :  900  \n 1st Qu.:  1200   NA's:67        1st Qu.: 1564   1st Qu.: 1705  \n Median :  1905                  Median : 1853   Median : 2334  \n Mean   : 20408                  Mean   : 4059   Mean   : 4355  \n 3rd Qu.: 14313                  3rd Qu.: 3444   3rd Qu.: 3839  \n Max.   :109000                  Max.   :17182   Max.   :16414  \n NA's   :60                      NA's   :59      NA's   :59     \n      ...9           ...10           ...11            ...12       \n Min.   :   80   Min.   :  165   Min.   :   250   Min.   :   206  \n 1st Qu.: 1773   1st Qu.:  854   1st Qu.:  2034   1st Qu.:  2637  \n Median : 2826   Median : 3216   Median :  4194   Median :  4000  \n Mean   : 4964   Mean   : 6979   Mean   : 17533   Mean   : 17788  \n 3rd Qu.: 5275   3rd Qu.: 5968   3rd Qu.: 12214   3rd Qu.: 11418  \n Max.   :19542   Max.   :32545   Max.   :120000   Max.   :130000  \n NA's   :58      NA's   :58      NA's   :57       NA's   :56      \n     ...13            ...14            ...15            ...16       \n Min.   :   230   Min.   :   280   Min.   :   502   Min.   :   663  \n 1st Qu.:  2582   1st Qu.:  3290   1st Qu.:  3664   1st Qu.:  6440  \n Median :  5235   Median :  6253   Median :  7096   Median : 12818  \n Mean   : 18163   Mean   : 19847   Mean   : 19192   Mean   : 26310  \n 3rd Qu.:  9915   3rd Qu.: 13618   3rd Qu.: 15092   3rd Qu.: 33881  \n Max.   :135000   Max.   :135000   Max.   :125000   Max.   :115000  \n NA's   :56       NA's   :56       NA's   :56       NA's   :55      \n     ...17            ...18            ...19            ...20       \n Min.   :  1385   Min.   :  1917   Min.   :  1918   Min.   :  1814  \n 1st Qu.: 10335   1st Qu.: 11645   1st Qu.:  9086   1st Qu.:  3000  \n Median : 14100   Median : 18387   Median : 15496   Median : 25000  \n Mean   : 27209   Mean   : 34844   Mean   : 33461   Mean   : 43042  \n 3rd Qu.: 34005   3rd Qu.: 35400   3rd Qu.: 36000   3rd Qu.: 58362  \n Max.   :118740   Max.   :135000   Max.   :140000   Max.   :169492  \n NA's   :55       NA's   :54       NA's   :54       NA's   :50      \n     ...21            ...22            ...23            ...24       \n Min.   :  1920   Min.   :  1921   Min.   :     7   Min.   :    15  \n 1st Qu.:  8665   1st Qu.: 13036   1st Qu.:  9500   1st Qu.:  6431  \n Median : 30949   Median : 28617   Median : 31177   Median : 41328  \n Mean   : 51136   Mean   : 51361   Mean   : 50311   Mean   : 55210  \n 3rd Qu.: 67111   3rd Qu.: 68661   3rd Qu.: 76509   3rd Qu.: 92675  \n Max.   :240966   Max.   :273737   Max.   :219164   Max.   :218000  \n NA's   :49       NA's   :48       NA's   :47       NA's   :45      \n     ...25            ...26            ...27            ...28       \n Min.   :    17   Min.   :   206   Min.   :   533   Min.   :   651  \n 1st Qu.:  8686   1st Qu.: 13651   1st Qu.: 19545   1st Qu.: 24836  \n Median : 35020   Median : 50952   Median : 53173   Median : 61151  \n Mean   : 58453   Mean   : 77586   Mean   : 87095   Mean   : 99954  \n 3rd Qu.: 88826   3rd Qu.:118958   3rd Qu.:130503   3rd Qu.:152692  \n Max.   :224211   Max.   :265500   Max.   :274209   Max.   :490430  \n NA's   :44       NA's   :45       NA's   :45       NA's   :45      \n     ...29            ...30            ...31            ...32       \n Min.   :   802   Min.   :   500   Min.   :   400   Min.   :   405  \n 1st Qu.: 34096   1st Qu.: 26106   1st Qu.: 35982   1st Qu.: 51995  \n Median : 76820   Median : 76822   Median : 88000   Median : 85000  \n Mean   :109988   Mean   :108078   Mean   :109788   Mean   :117479  \n 3rd Qu.:159144   3rd Qu.:149554   3rd Qu.:157693   3rd Qu.:156964  \n Max.   :460619   Max.   :461257   Max.   :458566   Max.   :461855  \n NA's   :45       NA's   :42       NA's   :42       NA's   :42      \n     ...33            ...34            ...35            ...36       \n Min.   :   357   Min.   :   386   Min.   :   275   Min.   :   300  \n 1st Qu.: 20356   1st Qu.: 11615   1st Qu.: 19791   1st Qu.: 15879  \n Median : 57338   Median : 51925   Median : 71901   Median : 83321  \n Mean   :109593   Mean   :103797   Mean   :119617   Mean   :125076  \n 3rd Qu.:153134   3rd Qu.:163980   3rd Qu.:218391   3rd Qu.:206316  \n Max.   :498289   Max.   :375000   Max.   :420000   Max.   :500000  \n NA's   :41       NA's   :39       NA's   :37       NA's   :35      \n     ...37            ...38             ...39            ...40       \n Min.   :   400   Min.   :    706   Min.   :  1130   Min.   :  1500  \n 1st Qu.: 20111   1st Qu.:  21213   1st Qu.: 20100   1st Qu.: 18000  \n Median :124697   Median : 124365   Median :121301   Median :116516  \n Mean   :173314   Mean   : 192804   Mean   :188685   Mean   :191680  \n 3rd Qu.:258988   3rd Qu.: 223413   3rd Qu.:224445   3rd Qu.:226741  \n Max.   :694098   Max.   :1041204   Max.   :954967   Max.   :911612  \n NA's   :33       NA's   :32        NA's   :31       NA's   :30      \n     ...41            ...42             ...43            ...44       \n Min.   :  1141   Min.   :      0   Min.   :     0   Min.   :     0  \n 1st Qu.: 18348   1st Qu.:  15506   1st Qu.:  7855   1st Qu.:  4570  \n Median :111185   Median : 124563   Median : 48144   Median : 18971  \n Mean   :201245   Mean   : 217969   Mean   : 97481   Mean   : 52331  \n 3rd Qu.:274769   3rd Qu.: 274002   3rd Qu.:124809   3rd Qu.: 60651  \n Max.   :950807   Max.   :1310101   Max.   :728706   Max.   :394140  \n NA's   :29       NA's   :26        NA's   :26       NA's   :26      \n     ...45            ...46            ...47             ...48        \n Min.   :     0   Min.   :     0   Min.   :      0   Min.   :      0  \n 1st Qu.:  3759   1st Qu.:  6397   1st Qu.:  16426   1st Qu.:  19374  \n Median : 19519   Median : 38624   Median : 124763   Median : 162563  \n Mean   : 62445   Mean   :108193   Mean   : 222159   Mean   : 264001  \n 3rd Qu.: 55707   3rd Qu.:130091   3rd Qu.: 308593   3rd Qu.: 376973  \n Max.   :534586   Max.   :750690   Max.   :1157930   Max.   :1204017  \n NA's   :25       NA's   :25       NA's   :25        NA's   :25       \n     ...49             ...50             ...51             ...52        \n Min.   :   1948   Min.   :   1949   Min.   :   1950   Min.   :   1951  \n 1st Qu.:  26703   1st Qu.:  37842   1st Qu.:  31636   1st Qu.:  36043  \n Median : 169063   Median : 200555   Median : 189286   Median : 224801  \n Mean   : 281803   Mean   : 325711   Mean   : 328327   Mean   : 371395  \n 3rd Qu.: 386848   3rd Qu.: 404359   3rd Qu.: 425890   3rd Qu.: 497083  \n Max.   :1469749   Max.   :1539641   Max.   :1843620   Max.   :1945100  \n NA's   :25        NA's   :25        NA's   :24        NA's   :24       \n     ...53             ...54             ...55             ...56        \n Min.   :   1952   Min.   :   1953   Min.   :   1954   Min.   :   1955  \n 1st Qu.:  49458   1st Qu.:  58464   1st Qu.:  59350   1st Qu.:  69100  \n Median : 312677   Median : 332835   Median : 337900   Median : 342200  \n Mean   : 425016   Mean   : 437933   Mean   : 452945   Mean   : 467239  \n 3rd Qu.: 564989   3rd Qu.: 590949   3rd Qu.: 581000   3rd Qu.: 642950  \n Max.   :2322152   Max.   :2250772   Max.   :2526900   Max.   :2581500  \n NA's   :24        NA's   :24        NA's   :24        NA's   :24       \n     ...57             ...58             ...59             ...60        \n Min.   :    500   Min.   :    600   Min.   :    700   Min.   :   1100  \n 1st Qu.:  62000   1st Qu.:  49975   1st Qu.:  57425   1st Qu.:  70650  \n Median : 303800   Median : 328900   Median : 340500   Median : 360450  \n Mean   : 488572   Mean   : 497117   Mean   : 519619   Mean   : 539682  \n 3rd Qu.: 669800   3rd Qu.: 692250   3rd Qu.: 711525   3rd Qu.: 778475  \n Max.   :2885800   Max.   :2943700   Max.   :3168900   Max.   :3162300  \n NA's   :22        NA's   :21        NA's   :21        NA's   :21       \n     ...61             ...62             ...63             ...64        \n Min.   :    600   Min.   :    600   Min.   :    300   Min.   :    700  \n 1st Qu.:  71800   1st Qu.:  83750   1st Qu.:  93450   1st Qu.: 103525  \n Median : 397700   Median : 415600   Median : 399000   Median : 410800  \n Mean   : 621101   Mean   : 644369   Mean   : 742316   Mean   : 740547  \n 3rd Qu.: 871600   3rd Qu.: 801450   3rd Qu.:1005450   3rd Qu.: 966825  \n Max.   :4528600   Max.   :4762100   Max.   :5209800   Max.   :5258700  \n NA's   :20        NA's   :20        NA's   :20        NA's   :19       \n     ...65             ...66             ...67             ...68        \n Min.   :    500   Min.   :    800   Min.   :    300   Min.   :   1200  \n 1st Qu.: 102325   1st Qu.: 118000   1st Qu.: 117900   1st Qu.: 146600  \n Median : 432250   Median : 480500   Median : 513000   Median : 516300  \n Mean   : 759212   Mean   : 852191   Mean   : 936444   Mean   : 913177  \n 3rd Qu.: 940675   3rd Qu.:1091300   3rd Qu.:1143800   3rd Qu.:1282800  \n Max.   :5321100   Max.   :5954900   Max.   :6466100   Max.   :6710100  \n NA's   :19        NA's   :18        NA's   :18        NA's   :18       \n     ...69             ...70             ...71             ...72        \n Min.   :   1600   Min.   :   1969   Min.   :   1970   Min.   :   1971  \n 1st Qu.: 147000   1st Qu.: 162600   1st Qu.: 183225   1st Qu.: 193875  \n Median : 578300   Median : 550300   Median : 611750   Median : 542550  \n Mean   : 967624   Mean   : 975214   Mean   :1018211   Mean   : 907303  \n 3rd Qu.:1540200   3rd Qu.:1299700   3rd Qu.:1367225   3rd Qu.:1306500  \n Max.   :6667100   Max.   :6331100   Max.   :6778500   Max.   :7173000  \n NA's   :18        NA's   :18        NA's   :17        NA's   :15       \n     ...73             ...74             ...75             ...76        \n Min.   :   1972   Min.   :   1973   Min.   :   1974   Min.   :   1975  \n 1st Qu.: 168896   1st Qu.: 196850   1st Qu.: 162775   1st Qu.: 242925  \n Median : 546286   Median : 500750   Median : 470750   Median : 558150  \n Mean   :1000728   Mean   : 960740   Mean   : 906212   Mean   : 994583  \n 3rd Qu.:1391299   3rd Qu.:1350050   3rd Qu.:1213925   3rd Qu.:1307375  \n Max.   :8034753   Max.   :7586300   Max.   :7807800   Max.   :8541500  \n NA's   :14        NA's   :13        NA's   :13        NA's   :13       \n     ...77             ...78             ...79             ...80        \n Min.   :   1976   Min.   :   1977   Min.   :   1978   Min.   :   1979  \n 1st Qu.: 215300   1st Qu.: 246000   1st Qu.: 276200   1st Qu.: 246900  \n Median : 625600   Median : 622300   Median : 663331   Median : 585678  \n Mean   :1068945   Mean   :1093285   Mean   :1073324   Mean   : 939412  \n 3rd Qu.:1312300   3rd Qu.:1375800   3rd Qu.:1321844   3rd Qu.:1400204  \n Max.   :8991500   Max.   :9173600   Max.   :8695534   Max.   :8019788  \n NA's   :12        NA's   :12        NA's   :11        NA's   :11       \n     ...81             ...82             ...83             ...84        \n Min.   :   1980   Min.   :   1981   Min.   :   1381   Min.   :   1983  \n 1st Qu.: 262219   1st Qu.: 264326   1st Qu.: 172287   1st Qu.: 164926  \n Median : 567420   Median : 592454   Median : 566807   Median : 577439  \n Mean   : 962913   Mean   :1020821   Mean   : 941126   Mean   : 958910  \n 3rd Qu.:1234220   3rd Qu.:1247455   3rd Qu.:1030484   3rd Qu.:1160008  \n Max.   :8440953   Max.   :8312884   Max.   :8177869   Max.   :8435475  \n NA's   :11        NA's   :11        NA's   :6         NA's   :6        \n     ...85             ...86             ...87             ...88         \n Min.   :   1075   Min.   :   1305   Min.   :   1085   Min.   :     230  \n 1st Qu.: 166610   1st Qu.: 183348   1st Qu.: 185612   1st Qu.:  203216  \n Median : 539476   Median : 505791   Median : 586668   Median :  651606  \n Mean   : 924798   Mean   : 925230   Mean   : 990115   Mean   : 1041905  \n 3rd Qu.:1142727   3rd Qu.:1139202   3rd Qu.:1228194   3rd Qu.: 1233212  \n Max.   :8508390   Max.   :9319290   Max.   :9836306   Max.   :10209841  \n NA's   :5         NA's   :4         NA's   :4         NA's   :4         \n     ...89             ...90             ...91             ...92        \n Min.   :   1258   Min.   :    822   Min.   :   1010   Min.   :   1154  \n 1st Qu.: 217918   1st Qu.: 238006   1st Qu.: 240466   1st Qu.: 212755  \n Median : 675397   Median : 600045   Median : 611375   Median : 679034  \n Mean   :1049132   Mean   :1061917   Mean   :1018962   Mean   :1083886  \n 3rd Qu.:1231204   3rd Qu.:1302215   3rd Qu.:1293538   3rd Qu.:1438690  \n Max.   :8770781   Max.   :8333553   Max.   :8151769   Max.   :8654459  \n NA's   :4         NA's   :4         NA's   :4         NA's   :4        \n     ...93             ...94             ...95             ...96        \n Min.   :   1992   Min.   :   1993   Min.   :   1726   Min.   :      0  \n 1st Qu.: 219461   1st Qu.: 214598   1st Qu.: 211855   1st Qu.: 241461  \n Median : 688742   Median : 666054   Median : 685031   Median : 663794  \n Mean   :1108917   Mean   :1141519   Mean   :1157714   Mean   :1197706  \n 3rd Qu.:1467198   3rd Qu.:1421256   3rd Qu.:1573088   3rd Qu.:1605836  \n Max.   :8931690   Max.   :9283848   Max.   :8628174   Max.   :9080420  \n NA's   :4         NA's   :4         NA's   :4         NA's   :4        \n     ...97             ...98             ...99             ...100        \n Min.   :   1996   Min.   :   1997   Min.   :   1998   Min.   :    1999  \n 1st Qu.: 285684   1st Qu.: 306023   1st Qu.: 271858   1st Qu.:  288709  \n Median : 644502   Median : 627720   Median : 604556   Median :  635736  \n Mean   :1198161   Mean   :1211313   Mean   :1204959   Mean   : 1195782  \n 3rd Qu.:1535120   3rd Qu.:1548693   3rd Qu.:1474971   3rd Qu.: 1443662  \n Max.   :9265667   Max.   :9965075   Max.   :9989395   Max.   :10283598  \n NA's   :5         NA's   :4         NA's   :4         NA's   :4         \n     ...101             ...102            ...103            ...104       \n Min.   :    2000   Min.   :   2001   Min.   :   1938   Min.   :      0  \n 1st Qu.:  257790   1st Qu.: 269938   1st Qu.: 237364   1st Qu.: 241347  \n Median :  605192   Median : 541787   Median : 558503   Median : 570953  \n Mean   : 1167022   Mean   :1133359   Mean   :1125762   Mean   :1097179  \n 3rd Qu.: 1467108   3rd Qu.:1377130   3rd Qu.:1401990   3rd Qu.:1323676  \n Max.   :10175812   Max.   :9197697   Max.   :9316420   Max.   :9366845  \n NA's   :4          NA's   :4         NA's   :3         NA's   :3        \n     ...105            ...106            ...107            ...108       \n Min.   :   2004   Min.   :   2005   Min.   :   1239   Min.   :    847  \n 1st Qu.: 258122   1st Qu.: 268943   1st Qu.: 246691   1st Qu.: 268616  \n Median : 549708   Median : 594893   Median : 569464   Median : 548004  \n Mean   :1112941   Mean   :1115680   Mean   :1040673   Mean   :1068904  \n 3rd Qu.:1363063   3rd Qu.:1424896   3rd Qu.:1260680   3rd Qu.:1299272  \n Max.   :9167046   Max.   :9192477   Max.   :9289215   Max.   :9372253  \n NA's   :4         NA's   :4         NA's   :3         NA's   :3        \n     ...109            ...110            ...111            ...112       \n Min.   :   1565   Min.   :   1879   Min.   :   2010   Min.   :   2011  \n 1st Qu.: 259539   1st Qu.: 221411   1st Qu.: 271609   1st Qu.: 270732  \n Median : 547580   Median : 568652   Median : 568426   Median : 550900  \n Mean   :1043258   Mean   :1078816   Mean   :1110641   Mean   :1072016  \n 3rd Qu.:1219177   3rd Qu.:1220559   3rd Qu.:1290286   3rd Qu.:1310031  \n Max.   :9044010   Max.   :9491437   Max.   :9463538   Max.   :9008830  \n NA's   :3         NA's   :3         NA's   :3         NA's   :3        \n     ...113            ...114            ...115             ...116        \n Min.   :   2012   Min.   :   2013   Min.   :       0   Min.   :       0  \n 1st Qu.: 243314   1st Qu.: 236605   1st Qu.:  262790   1st Qu.:  282101  \n Median : 518568   Median : 526974   Median :  538970   Median :  596116  \n Mean   :1110429   Mean   :1080282   Mean   : 1155141   Mean   : 1254802  \n 3rd Qu.:1323217   3rd Qu.:1315336   3rd Qu.: 1427298   3rd Qu.: 1473670  \n Max.   :9685829   Max.   :9354695   Max.   :10099276   Max.   :10712674  \n NA's   :3         NA's   :3         NA's   :3          NA's   :3         \n     ...117             ...118             ...119             ...120        \n Min.   :    2016   Min.   :    2017   Min.   :    2018   Min.   :    2019  \n 1st Qu.:  320378   1st Qu.:  304206   1st Qu.:  291636   1st Qu.:  325694  \n Median :  630326   Median :  667870   Median :  650660   Median :  681872  \n Mean   : 1369082   Mean   : 1396772   Mean   : 1370565   Mean   : 1422075  \n 3rd Qu.: 1554654   3rd Qu.: 1544675   3rd Qu.: 1667333   3rd Qu.: 1680013  \n Max.   :11312786   Max.   :11338893   Max.   :11421200   Max.   :12547743  \n NA's   :3          NA's   :3          NA's   :3          NA's   :3         \n     ...121             ...122        \n Min.   :    2020   Min.   :    2021  \n 1st Qu.:  162119   1st Qu.:  292505  \n Median :  454968   Median :  707328  \n Mean   : 1061464   Mean   : 1441467  \n 3rd Qu.: 1265616   3rd Qu.: 1713756  \n Max.   :12095720   Max.   :14161548  \n NA's   :3          NA's   :3         \n\nclass(orig_nps)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nnames(orig_nps) #Exploring columns names, they look to be all numbers\n\n  [1] \"...1\"   \"...2\"   \"...3\"   \"...4\"   \"...5\"   \"...6\"   \"...7\"   \"...8\"  \n  [9] \"...9\"   \"...10\"  \"...11\"  \"...12\"  \"...13\"  \"...14\"  \"...15\"  \"...16\" \n [17] \"...17\"  \"...18\"  \"...19\"  \"...20\"  \"...21\"  \"...22\"  \"...23\"  \"...24\" \n [25] \"...25\"  \"...26\"  \"...27\"  \"...28\"  \"...29\"  \"...30\"  \"...31\"  \"...32\" \n [33] \"...33\"  \"...34\"  \"...35\"  \"...36\"  \"...37\"  \"...38\"  \"...39\"  \"...40\" \n [41] \"...41\"  \"...42\"  \"...43\"  \"...44\"  \"...45\"  \"...46\"  \"...47\"  \"...48\" \n [49] \"...49\"  \"...50\"  \"...51\"  \"...52\"  \"...53\"  \"...54\"  \"...55\"  \"...56\" \n [57] \"...57\"  \"...58\"  \"...59\"  \"...60\"  \"...61\"  \"...62\"  \"...63\"  \"...64\" \n [65] \"...65\"  \"...66\"  \"...67\"  \"...68\"  \"...69\"  \"...70\"  \"...71\"  \"...72\" \n [73] \"...73\"  \"...74\"  \"...75\"  \"...76\"  \"...77\"  \"...78\"  \"...79\"  \"...80\" \n [81] \"...81\"  \"...82\"  \"...83\"  \"...84\"  \"...85\"  \"...86\"  \"...87\"  \"...88\" \n [89] \"...89\"  \"...90\"  \"...91\"  \"...92\"  \"...93\"  \"...94\"  \"...95\"  \"...96\" \n [97] \"...97\"  \"...98\"  \"...99\"  \"...100\" \"...101\" \"...102\" \"...103\" \"...104\"\n[105] \"...105\" \"...106\" \"...107\" \"...108\" \"...109\" \"...110\" \"...111\" \"...112\"\n[113] \"...113\" \"...114\" \"...115\" \"...116\" \"...117\" \"...118\" \"...119\" \"...120\"\n[121] \"...121\" \"...122\"\n\nnps1<-orig_nps[-c(1,3,6, 118:122)] #Here I will remove the columns that I do not need by selecting the column number\nnps2<-nps1[-c(1,2,65),] #I am going to clean the data a bit more by removing the two rows at the top of the dataset\nprint(nps2) #Much more succinct\n\n# A tibble: 64 × 114\n   ...2   ...4  ...5  ...7  ...8  ...9 ...10 ...11 ...12 ...13 ...14 ...15 ...16\n   <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Park…  1904  1905  1906  1907  1908  1909  1910  1911  1912  1913  1914  1915\n 2 Dena…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 3 Gate…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 4 Glac…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 5 Katm…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 6 Kena…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 7 Kobu…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 8 Lake…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n 9 Wran…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n10 Arch…    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA    NA\n# … with 54 more rows, and 101 more variables: ...17 <dbl>, ...18 <dbl>,\n#   ...19 <dbl>, ...20 <dbl>, ...21 <dbl>, ...22 <dbl>, ...23 <dbl>,\n#   ...24 <dbl>, ...25 <dbl>, ...26 <dbl>, ...27 <dbl>, ...28 <dbl>,\n#   ...29 <dbl>, ...30 <dbl>, ...31 <dbl>, ...32 <dbl>, ...33 <dbl>,\n#   ...34 <dbl>, ...35 <dbl>, ...36 <dbl>, ...37 <dbl>, ...38 <dbl>,\n#   ...39 <dbl>, ...40 <dbl>, ...41 <dbl>, ...42 <dbl>, ...43 <dbl>,\n#   ...44 <dbl>, ...45 <dbl>, ...46 <dbl>, ...47 <dbl>, ...48 <dbl>, …\n\n\nWhile our dataset is cleaner and more succinct, there are no column or row names\n\nnps3<- nps2 %>% #Using first row of dataset for column names \n    row_to_names(row_number=1)\n\nnps4<- nps3[!is.na(nps3$`Park Name`),] #Removing NA from Park Name column\n\nnps5<- nps4 %>% #Converting dataset from wide to long \n  pivot_longer(\n    cols = c(-`Park Name`), \n    names_to = \"year\", \n    values_to = \"visitors\",\n    values_drop_na = TRUE)\n\nnps_clean<- nps5 %>% #Ranking the parks \n    group_by(year) %>%\n    mutate(Rank = order(order(visitors, decreasing=TRUE))) %>%\n    ungroup() %>%\n  rename(park_name = `Park Name`)\n\n\n\nBeginning Data Visualization\n\nlibrary(ggthemes)\n\nnps_clean$year<-as.numeric(as.character(nps_clean$year)) #Making year numeric\n\n#Creating df for each of the top parks to overlay on nps_g1\nGSM<- nps_clean [which (nps_clean$park_name==\"Great Smoky Mountains NP\"),]  \n\nGC<- nps_clean [which (nps_clean$park_name==\"Grand Canyon NP\"),] \n\nRMNP<- nps_clean [which (nps_clean$park_name==\"Rocky Mountain NP\"),] \n\nYNP<- nps_clean [which (nps_clean$park_name==\"Yosemite NP\"),]\n\nYSNP<- nps_clean [which (nps_clean$park_name==\"Yellowstone NP\"),] \n\nZNP<- nps_clean [which (nps_clean$park_name==\"Zion NP\"),] \n\nANP<- nps_clean [which (nps_clean$park_name==\"Acadia NP\"),] \n    \nHSNP<- nps_clean [which (nps_clean$park_name==\"Hot Springs NP\"),]\n\nDNP<- nps_clean [which (nps_clean$park_name==\"Denali NP & PRES\"),]\n\nCCNP<- nps_clean [which (nps_clean$park_name==\"Carlsbad Caverns NP\"),]\n\nGBNP<- nps_clean [which (nps_clean$park_name==\"Great Basin NP\"),]\n\n\nnps_g1<- nps_clean %>% ggplot() +geom_line( \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"grey\") +\n  theme_fivethirtyeight() +\n  theme(legend.position = \"none\") + \n  scale_y_reverse(breaks=seq(50,1,-25), limits=c(62,-1)) + #setting y axis\n  scale_x_continuous(breaks=seq(1925,2000,25),limits=c(1904,2030)) + #setting y axis \n  xlab(\"Year\") + \n  ylab(\"Rank\") +\n  labs(title= \"The most popular national parks\",\n       subtitle= \"National parks ranked by number of visitors in a given year\") + \n  geom_line(data=GSM, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"darkolivegreen\") +\n    annotate(\"text\", x = 2000, y =0, label = \"Great Smoky Mountains\", color = \"darkolivegreen\", fontface=2, size=3) +\n  geom_line(data=GC, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"deepskyblue4\") +\n    annotate(\"text\", x = 2020, y =0.7, label = \"Grand Canyon\", color = \"deepskyblue4\", fontface=2, size=3) + \n  geom_line(data=RMNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"cyan4\")+\n    annotate(\"text\", x = 2021, y =2, label = \"Rocky Mountain\", color = \"cyan4\", fontface=2, size= 3) +\n  geom_line(data=YNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"chartreuse4\") +\n  annotate(\"text\", x = 2020, y =3.1, label = \"Yosemite\", color = \"chartreuse4\", fontface=2, size= 3) +\n  geom_line(data=YSNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"orange2\") +\n  annotate(\"text\", x = 2019, y =4.1, label = \"Yellowstone\", color = \"orange2\", fontface=2, size= 3) +\n  geom_line(data=ZNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"tomato\") +\n  geom_line(data=ANP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"gold1\") +\n  geom_line(data=HSNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"plum3\") +\n  geom_line(data=DNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"mediumpurple1\") +\n  geom_line(data=CCNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"lightskyblue2\") +\n  geom_line(data=GBNP, \n  aes(x = year, y = Rank, color = park_name, group = park_name), color=\"maroon2\")\n  \n\nnps_g1\n\n\n\n\nHere is an update on my data visualization! 3/10/2023 I think I bit off a bit more than I could chew with this graph. But, I am pretty proud of how far I got with it. To make things a little more cohesive, I overrode the color to grey. I plan on continuing work on it tonight with the hopes of adding the 11 parks in color. It is definitely a work in progress!"
=======
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
=======
    "objectID": "fluanalysis/code/modeleval.html",
    "href": "fluanalysis/code/modeleval.html",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "",
    "text": "Let’s Begin with Model Evaluation"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#but-first-lets-load-some-packages",
    "href": "fluanalysis/code/modeleval.html#but-first-lets-load-some-packages",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "But first, let’s load some packages…",
    "text": "But first, let’s load some packages…\n\nlibrary(dplyr) #Data wrangling \n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #Helps with data wrangling\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\nlibrary(here) #Setting paths\n\nWarning: package 'here' was built under R version 4.2.2\n\n\nhere() starts at C:/Users/Sara/Documents/Documents/Current Classes/MADA/kimberlyperez-MADA-portfolio\n\nlibrary(tidyverse) #Data transformation\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggplot2) #Graphs/Visualization\nlibrary(tidymodels) #For modeling\n\nWarning: package 'tidymodels' was built under R version 4.2.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n\n\nWarning: package 'broom' was built under R version 4.2.2\n\n\nWarning: package 'dials' was built under R version 4.2.2\n\n\nWarning: package 'scales' was built under R version 4.2.2\n\n\nWarning: package 'infer' was built under R version 4.2.2\n\n\nWarning: package 'modeldata' was built under R version 4.2.2\n\n\nWarning: package 'parsnip' was built under R version 4.2.2\n\n\nWarning: package 'recipes' was built under R version 4.2.2\n\n\nWarning: package 'rsample' was built under R version 4.2.2\n\n\nWarning: package 'tune' was built under R version 4.2.2\n\n\nWarning: package 'workflows' was built under R version 4.2.2\n\n\nWarning: package 'workflowsets' was built under R version 4.2.2\n\n\nWarning: package 'yardstick' was built under R version 4.2.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-recipe-with-all-symptoms",
    "href": "fluanalysis/code/modeleval.html#create-recipe-with-all-symptoms",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "Create recipe with all symptoms",
    "text": "Create recipe with all symptoms\nFollowing the same steps as above:\n\n#create recipe using all symptoms as predictors of body temp\nflu_recBTAS <- \n  recipe(BodyTemp ~ ., data = train_data_flu)\n\n#set model\nln_mod <- linear_reg() %>% \n  set_engine(\"glm\")\n\n#create work flow\nflu_wflowBTAS <-\n  workflow() %>% \n  add_model(ln_mod) %>% \n  add_recipe(flu_recBTAS)\n\n#create fitted model\nflu_fitBTAS <-\n  flu_wflowBTAS %>% \n  fit(data = train_data_flu)\n\n#check fitted model\nflu_fitBTAS %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)           97.8        0.347   282.    0      \n 2 SwollenLymphNodesYes  -0.124      0.105    -1.17  0.241  \n 3 ChestCongestionYes     0.0731     0.112     0.655 0.513  \n 4 ChillsSweatsYes        0.140      0.148     0.949 0.343  \n 5 NasalCongestionYes    -0.183      0.131    -1.39  0.164  \n 6 CoughYNYes             0.353      0.268     1.31  0.189  \n 7 SneezeYes             -0.297      0.110    -2.70  0.00706\n 8 FatigueYes             0.360      0.185     1.94  0.0528 \n 9 SubjectiveFeverYes     0.361      0.116     3.11  0.00196\n10 HeadacheYes            0.0332     0.142     0.233 0.816  \n# … with 28 more rows\n\n\nHere, we can see the fitted model predicts Body Temperature from all symptoms with most predictors not being statistically significant. Estimates cannot be directly compared without standardizing the variables."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#predictions-from-trained-model",
    "href": "fluanalysis/code/modeleval.html#predictions-from-trained-model",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "Predictions from trained model",
    "text": "Predictions from trained model\nWe can also make predictions using the flu_fitBTAS model and the test_data_flu.\n\n#create predictions\nflu_augBTAS <- augment(flu_fitBTAS, test_data_flu)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n#check RMSE as metric for model performance\nflu_augBTAS %>% \n  rmse(truth = BodyTemp, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.23\n\n\nThe root mean square error has an estimate of 1.230, indicating this would not be a good model for the data.\nWe can also use the train_data_flu data to make predictions.\n\n#predict from training data\nflu_augRN2 <- augment(flu_fitBTAS, train_data_flu)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n#generate RMSE for model performance\nflu_augRN2 %>% \n  rmse(truth = BodyTemp, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.08\n\n\nThe RMSE is lower for the train data, but still not an ideal value."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-recipe-with-runny-nose",
    "href": "fluanalysis/code/modeleval.html#create-recipe-with-runny-nose",
>>>>>>> Stashed changes
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "Create recipe with Runny Nose",
    "text": "Create recipe with Runny Nose\nFollow the same steps with RunnyNose as the predictor.\n\n#create recipe using RunnyNose as predictor of body temp\nflu_recBTRN <- \n  recipe(BodyTemp ~ RunnyNose, data = train_data_flu)\n\n#set model\nln_mod <- linear_reg() %>% \n  set_engine(\"glm\")\n\n#create work flow\nflu_wflowBTRN <-\n  workflow() %>% \n  add_model(ln_mod) %>% \n  add_recipe(flu_recBTRN)\n\n#create fitted model\nflu_fitBTRN <-\n  flu_wflowBTRN %>% \n  fit(data = train_data_flu)\n\n#check fitted model\nflu_fitBTRN %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0931   1065.    0     \n2 RunnyNoseYes   -0.246    0.110      -2.24  0.0252\n\n\nHere, the model predicts Body Temperature from Runny Nose. Having a runny nose appears to predict a lower body temperature by 0.246 degrees."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#predictions-from-trained-model-1",
    "href": "fluanalysis/code/modeleval.html#predictions-from-trained-model-1",
    "title": "Flu Analysis Data: Model Evaluation",
<<<<<<< HEAD
    "section": "But first, let’s load some packages…",
    "text": "But first, let’s load some packages…\n\nlibrary(dplyr) #Data wrangling \n\nWarning: package 'dplyr' was built under R version 4.2.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #Helps with data wrangling\n\nWarning: package 'tidyr' was built under R version 4.2.2\n\nlibrary(here) #Setting paths\n\nWarning: package 'here' was built under R version 4.2.2\n\n\nhere() starts at C:/Users/Sara/Documents/Documents/Current Classes/MADA/kimberlyperez-MADA-portfolio\n\nlibrary(tidyverse) #Data transformation\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ readr   2.1.3     ✔ forcats 0.5.2\n\n\nWarning: package 'ggplot2' was built under R version 4.2.2\n\n\nWarning: package 'readr' was built under R version 4.2.2\n\n\nWarning: package 'purrr' was built under R version 4.2.2\n\n\nWarning: package 'stringr' was built under R version 4.2.2\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggplot2) #Graphs/Visualization\nlibrary(tidymodels) #For modeling\n\nWarning: package 'tidymodels' was built under R version 4.2.2\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.0.1     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n\n\nWarning: package 'broom' was built under R version 4.2.2\n\n\nWarning: package 'dials' was built under R version 4.2.2\n\n\nWarning: package 'scales' was built under R version 4.2.2\n\n\nWarning: package 'infer' was built under R version 4.2.2\n\n\nWarning: package 'modeldata' was built under R version 4.2.2\n\n\nWarning: package 'parsnip' was built under R version 4.2.2\n\n\nWarning: package 'recipes' was built under R version 4.2.2\n\n\nWarning: package 'rsample' was built under R version 4.2.2\n\n\nWarning: package 'tune' was built under R version 4.2.2\n\n\nWarning: package 'workflows' was built under R version 4.2.2\n\n\nWarning: package 'workflowsets' was built under R version 4.2.2\n\n\nWarning: package 'yardstick' was built under R version 4.2.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/"
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-recipe-with-all-symptoms",
    "href": "fluanalysis/code/modeleval.html#create-recipe-with-all-symptoms",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "Create recipe with all symptoms",
    "text": "Create recipe with all symptoms\nFollowing the same steps as above:\n\n#create recipe using all symptoms as predictors of body temp\nflu_recBTAS <- \n  recipe(BodyTemp ~ ., data = train_data_flu)\n\n#set model\nln_mod <- linear_reg() %>% \n  set_engine(\"glm\")\n\n#create work flow\nflu_wflowBTAS <-\n  workflow() %>% \n  add_model(ln_mod) %>% \n  add_recipe(flu_recBTAS)\n\n#create fitted model\nflu_fitBTAS <-\n  flu_wflowBTAS %>% \n  fit(data = train_data_flu)\n\n#check fitted model\nflu_fitBTAS %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 38 × 5\n   term                 estimate std.error statistic p.value\n   <chr>                   <dbl>     <dbl>     <dbl>   <dbl>\n 1 (Intercept)           97.8        0.347   282.    0      \n 2 SwollenLymphNodesYes  -0.124      0.105    -1.17  0.241  \n 3 ChestCongestionYes     0.0731     0.112     0.655 0.513  \n 4 ChillsSweatsYes        0.140      0.148     0.949 0.343  \n 5 NasalCongestionYes    -0.183      0.131    -1.39  0.164  \n 6 CoughYNYes             0.353      0.268     1.31  0.189  \n 7 SneezeYes             -0.297      0.110    -2.70  0.00706\n 8 FatigueYes             0.360      0.185     1.94  0.0528 \n 9 SubjectiveFeverYes     0.361      0.116     3.11  0.00196\n10 HeadacheYes            0.0332     0.142     0.233 0.816  \n# … with 28 more rows\n\n\nHere, we can see the fitted model predicts Body Temperature from all symptoms with most predictors not being statistically significant. Estimates cannot be directly compared without standardizing the variables."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#predictions-from-trained-model",
    "href": "fluanalysis/code/modeleval.html#predictions-from-trained-model",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "Predictions from trained model",
    "text": "Predictions from trained model\nWe can also make predictions using the flu_fitBTAS model and the test_data_flu.\n\n#create predictions\nflu_augBTAS <- augment(flu_fitBTAS, test_data_flu)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n#check RMSE as metric for model performance\nflu_augBTAS %>% \n  rmse(truth = BodyTemp, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.23\n\n\nThe root mean square error has an estimate of 1.230, indicating this would not be a good model for the data.\nWe can also use the train_data_flu data to make predictions.\n\n#predict from training data\nflu_augRN2 <- augment(flu_fitBTAS, train_data_flu)\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\n#generate RMSE for model performance\nflu_augRN2 %>% \n  rmse(truth = BodyTemp, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.08\n\n\nThe RMSE is lower for the train data, but still not an ideal value."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#create-recipe-with-runny-nose",
    "href": "fluanalysis/code/modeleval.html#create-recipe-with-runny-nose",
    "title": "Flu Analysis Data: Model Evaluation",
    "section": "Create recipe with Runny Nose",
    "text": "Create recipe with Runny Nose\nFollow the same steps with RunnyNose as the predictor.\n\n#create recipe using RunnyNose as predictor of body temp\nflu_recBTRN <- \n  recipe(BodyTemp ~ RunnyNose, data = train_data_flu)\n\n#set model\nln_mod <- linear_reg() %>% \n  set_engine(\"glm\")\n\n#create work flow\nflu_wflowBTRN <-\n  workflow() %>% \n  add_model(ln_mod) %>% \n  add_recipe(flu_recBTRN)\n\n#create fitted model\nflu_fitBTRN <-\n  flu_wflowBTRN %>% \n  fit(data = train_data_flu)\n\n#check fitted model\nflu_fitBTRN %>% \n  extract_fit_parsnip() %>% \n  tidy()\n\n# A tibble: 2 × 5\n  term         estimate std.error statistic p.value\n  <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    99.1      0.0931   1065.    0     \n2 RunnyNoseYes   -0.246    0.110      -2.24  0.0252\n\n\nHere, the model predicts Body Temperature from Runny Nose. Having a runny nose appears to predict a lower body temperature by 0.246 degrees."
  },
  {
    "objectID": "fluanalysis/code/modeleval.html#predictions-from-trained-model-1",
    "href": "fluanalysis/code/modeleval.html#predictions-from-trained-model-1",
    "title": "Flu Analysis Data: Model Evaluation",
<<<<<<< HEAD
    "section": "But first, let’s load some packages…",
    "text": "But first, let’s load some packages…\n\nlibrary(dplyr) #Data wrangling \n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #Helps with data wrangling\nlibrary(here) #Setting paths\n\nhere() starts at C:/GitHub/MADA/kimberlyperez-MADA-portfolio\n\nlibrary(tidyverse) #Data transformation\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ ggplot2 3.4.0     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 0.5.2\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(ggplot2) #Graphs/Visualization\nlibrary(tidymodels) #For modeling\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tune         1.0.1\n✔ infer        1.0.4     ✔ workflows    1.1.2\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.0\n✔ parsnip      1.0.3     ✔ yardstick    1.1.0\n✔ recipes      1.0.4     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html",
    "href": "fluanalysis/code/machinelearning.html",
    "title": "Flu Analysis: Machine Learning",
    "section": "",
    "text": "library(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(glmnet)\n\nWarning: package 'glmnet' was built under R version 4.2.3\n\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\nlibrary(here)\n\nhere() starts at C:/GitHub/MADA/kimberlyperez-MADA-portfolio\n\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.2.3\n\nlibrary(rpart)\nlibrary(rpart.plot)\n\nWarning: package 'rpart.plot' was built under R version 4.2.3\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.0.0 ──\n\n\n✔ broom        1.0.2     ✔ rsample      1.1.1\n✔ dials        1.1.0     ✔ tibble       3.1.8\n✔ infer        1.0.4     ✔ tidyr        1.3.0\n✔ modeldata    1.1.0     ✔ tune         1.0.1\n✔ parsnip      1.0.3     ✔ workflows    1.1.2\n✔ purrr        1.0.1     ✔ workflowsets 1.0.0\n✔ recipes      1.0.4     ✔ yardstick    1.1.0\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard()  masks scales::discard()\n✖ tidyr::expand()   masks Matrix::expand()\n✖ dplyr::filter()   masks stats::filter()\n✖ dplyr::lag()      masks stats::lag()\n✖ tidyr::pack()     masks Matrix::pack()\n✖ dials::prune()    masks rpart::prune()\n✖ recipes::step()   masks stats::step()\n✖ tidyr::unpack()   masks Matrix::unpack()\n✖ recipes::update() masks Matrix::update(), stats::update()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(tidyverse)\n\n── Attaching packages\n───────────────────────────────────────\ntidyverse 1.3.2 ──\n\n\n✔ readr   2.1.4     ✔ forcats 0.5.2\n✔ stringr 1.5.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ tidyr::expand()     masks Matrix::expand()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ tidyr::pack()       masks Matrix::pack()\n✖ readr::spec()       masks yardstick::spec()\n✖ tidyr::unpack()     masks Matrix::unpack()\n\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\n\n\n\nflu<-readRDS(here(\"fluanalysis\",\"processed_data\", \"SympAct_cleaned.rds\")) #Loading in the data\n\nglimpse(flu) #Looking at the Data \n\nRows: 730\nColumns: 32\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …"
  },
  {
    "objectID": "fluanalysis/code/machinelearning.html#reading-in-my-cleaned-data",
    "href": "fluanalysis/code/machinelearning.html#reading-in-my-cleaned-data",
    "title": "Machine Learning",
    "section": "Reading in my Cleaned Data",
    "text": "Reading in my Cleaned Data\n\nflu_ME<-readRDS(here(\"fluanalysis\",\"processed_data\", \"SympAct_cleaned.rds\")) #Loading in the data\n\nglimpse(flu_ME) #Looking at the Data \n\nRows: 730\nColumns: 32\n$ SwollenLymphNodes <fct> Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y…\n$ ChestCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ ChillsSweats      <fct> No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, …\n$ NasalCongestion   <fct> No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y…\n$ CoughYN           <fct> Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, …\n$ Sneeze            <fct> No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, …\n$ Fatigue           <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ SubjectiveFever   <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes…\n$ Headache          <fct> Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes…\n$ Weakness          <fct> Mild, Severe, Severe, Severe, Moderate, Moderate, Mi…\n$ WeaknessYN        <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ CoughIntensity    <fct> Severe, Severe, Mild, Moderate, None, Moderate, Seve…\n$ CoughYN2          <fct> Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes…\n$ Myalgia           <fct> Mild, Severe, Severe, Severe, Mild, Moderate, Mild, …\n$ MyalgiaYN         <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye…\n$ RunnyNose         <fct> No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No…\n$ AbPain            <fct> No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N…\n$ ChestPain         <fct> No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, …\n$ Diarrhea          <fct> No, No, No, No, No, Yes, No, No, No, No, No, No, No,…\n$ EyePn             <fct> No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye…\n$ Insomnia          <fct> No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y…\n$ ItchyEye          <fct> No, No, No, No, No, No, No, No, No, No, No, No, Yes,…\n$ Nausea            <fct> No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y…\n$ EarPn             <fct> No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y…\n$ Hearing           <fct> No, Yes, No, No, No, No, No, No, No, No, No, No, No,…\n$ Pharyngitis       <fct> Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, …\n$ Breathless        <fct> No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, …\n$ ToothPn           <fct> No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N…\n$ Vision            <fct> No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ Vomit             <fct> No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N…\n$ Wheeze            <fct> No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N…\n$ BodyTemp          <dbl> 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, …"
=======
    "section": "Predictions from trained model",
    "text": "Predictions from trained model\n\n#create predictions\nflu_augBTRN <- augment(flu_fitBTRN, test_data_flu)\n\n#check RMSE as metric for model performance\nflu_augBTRN %>% \n  rmse(truth = BodyTemp, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.30\n\n\nThe RMSE is similar to the all symptoms model with an estimate of 1.299.\n\n#predict from training data\nflu_augRN3 <- augment(flu_fitBTRN, train_data_flu)\n\n#generate RMSE for model performance\nflu_augRN3 %>% \n  rmse(truth = BodyTemp, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard        1.15\n\n\nUsing the train_data_flu dataset to predict, the RMSE is lower at 1.149. None of these models appear to be productive at predicting body temperature."
>>>>>>> main
<<<<<<< Updated upstream
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
  }
]