---
title: "Brian McKay's Flu Analysis Data: Model Evaluation"
editor: visual
---

**Let's Begin with Model Evaluation**

## **But first, let's load some packages...**

```{r}
library(dplyr) #Data wrangling 
library(tidyr) #Helps with data wrangling
library(here) #Setting paths
library(tidyverse) #Data transformation
library(ggplot2) #Graphs/Visualization
library(tidymodels) #For modeling
```

# **1. Reading in my Cleaned Data**

```{r}
flu_ME<-readRDS(here("fluanalysis","processed_data", "SympAct_cleaned.rds")) #Loading in the data

glimpse(flu_ME) #Looking at the Data 
```

# **2. Splitting the Data**

```{r}
set.seed(321)
data_split_ME<- initial_split(flu_ME, prop=3/4)

train_data_flu<- training(data_split_ME)
test_data_flu<- testing(data_split_ME)
```

# **3. Fitting a Model with a Recipe \[Trained Data\]**

```{r}
#Creating the recipe 
flu_recipe<- recipe(Nausea ~ ., data=train_data_flu)
```

# **4. Workflow Creation \[Trained Data\]**

```{r}
#Now Let's set a model
log_flu<- logistic_reg() %>%
  set_engine("glm")

#Creating Workflow
flu_WF<- workflow() %>% 
  add_model (log_flu) %>%
  add_recipe(flu_recipe)

#Creation of Single Function
flu_fit<- 
  flu_WF %>% 
  fit(data= train_data_flu)

#Extracting 
flu_fit %>%
  extract_fit_parsnip() %>%
  tidy()

#Predicting 
predict(flu_fit, train_data_flu)

pred_flufit<- augment(flu_fit, train_data_flu)

pred_flufit %>% 
  select(Nausea, .pred_No, .pred_Yes)
```

# **5. ROC Curve (1) \[Trained Data\]**

```{r}
pred_flufit %>% #Cool!
  roc_curve(truth= Nausea, .pred_No) %>%
  autoplot()
```

**Let's check the ROC Curve (1) performance**

```{r}
pred_flufit %>%
  roc_auc(truth= Nausea, .pred_No) #Sitting at 0.78 ROC-AUC seems to be useful 
```

**ROC Curve (2) \[Trained Data\]**

```{r}
pred_flufit %>% 
  roc_curve(truth= Nausea, .pred_Yes) %>%
  autoplot()
```

**Let's check the ROC Curve (2) performance**

```{r}
pred_flufit %>%
  roc_auc(truth= Nausea, .pred_Yes) #Please note the PREDICTOR- Sitting at 0.22 ROC-AUC seems to be no good.
```

# **6. Let's Do it Again with the Test Data!**

```{r}
#Creating the recipe 
flu_recipe_test<- recipe(Nausea ~ ., data=test_data_flu)
```

**Workflow Creation \[Test Data\]**

```{r}
#Now Let's set a model
log_flu_test<- logistic_reg() %>%
  set_engine("glm")

#Creating Workflow
flu_WF_test<- workflow() %>% 
  add_model (log_flu_test) %>%
  add_recipe(flu_recipe_test)

#Creation of Single Function
flu_fit_test<- 
  flu_WF_test %>% 
  fit(data= test_data_flu)

#Extracting 
flu_fit_test %>%
  extract_fit_parsnip() %>%
  tidy()

#Predicting 
predict(flu_fit_test, test_data_flu)

pred_flufit_test<- augment(flu_fit_test, test_data_flu)

pred_flufit_test %>% 
  select(Nausea, .pred_No, .pred_Yes)
```

**ROC Curve (1) \[Test Data\]**

```{r}
pred_flufit_test %>% #Let's make the curve
  roc_curve(truth= Nausea, .pred_No) %>%
  autoplot()
```

**Let's check the ROC Curve (1) performance**

```{r}
pred_flufit_test %>% #Let's check the performance <0.5= not useful, ~0.7= useful, 1= perfect
  roc_auc(truth= Nausea, .pred_No) #Sitting at 0.86 ROC-AUC is useful and the test data performers better than the trained.
```

**ROC Curve (2) \[Test Data\]**

```{r}
pred_flufit_test %>% 
  roc_curve(truth= Nausea, .pred_Yes) %>%
  autoplot()
```

**Let's check the ROC Curve (2) performance**

```{r}
pred_flufit_test %>%
  roc_auc(truth= Nausea, .pred_Yes) #Please note the PREDICTOR- Sitting at 0.14 ROC-AUC seems to be no good.
```

**7. Alternative Model with Categorical Outcome**

**I. Splitting the Data**

```{r}
set.seed(321)
data_split_RN<- initial_split(flu_ME, prop=3/4)

train_data_RN<- training(data_split_RN)
test_data_RN<- testing(data_split_RN)
```

**II. Fitting a Model with a Recipe \[Trained Data\]**

```{r}
#Creating the recipe 
flu_recipe_RN<- recipe(Nausea ~ RunnyNose, data=train_data_RN)
```

**III. Workflow Creation \[Trained Data\]**

```{r}
#Now Let's set a model
log_RN<- logistic_reg() %>%
  set_engine("glm")

#Creating Workflow
flu_WF_RN<- workflow() %>% 
  add_model (log_RN) %>%
  add_recipe(flu_recipe_RN)

#Creation of Single Function
flu_fit_RN<- 
  flu_WF_RN %>% 
  fit(data= train_data_RN)

#Extracting 
flu_fit_RN %>%
  extract_fit_parsnip() %>%
  tidy()
```

```{r}
#Predicting 
predict(flu_fit_RN, train_data_RN)

pred_RNfit<- augment(flu_fit_RN, train_data_RN)

pred_RNfit %>% 
  select(Nausea, .pred_No, .pred_Yes)
```

**IV. ROC Curve (1) \[Runny Nose: Trained Data\]**

```{r}
pred_RNfit %>% 
  roc_curve(truth= Nausea, .pred_No) %>%
  autoplot()
```

**V. ROC Curve Performance \[Runny Nose: Trained Data\]**

```{r}
pred_RNfit %>% #Let's check the performance <0.5= not useful, ~0.7= useful, 1= perfect
  roc_auc(truth= Nausea, .pred_No) #Sitting at 0.51 ROC-AUC is not useful, performers worse than the above.
```

**IIIa. Workflow Creation \[Test Data\]**

```{r}
#Now Let's set a model
log_RNTest<- logistic_reg() %>%
  set_engine("glm")

#Creating Workflow
flu_WF_RNTest<- workflow() %>% 
  add_model (log_RNTest) %>%
  add_recipe(flu_recipe_RN)

#Creation of Single Function
flu_fit_RNTest<- 
  flu_WF_RNTest %>% 
  fit(data= test_data_RN)

#Extracting 
flu_fit_RNTest %>%
  extract_fit_parsnip() %>%
  tidy()
```

```{r}
#Predicting 
predict(flu_fit_RNTest, test_data_RN)

pred_RNfitTest<- augment(flu_fit_RNTest, test_data_RN)

pred_RNfitTest %>% 
  select(Nausea, .pred_No, .pred_Yes)
```

**IVa. ROC Curve \[Runny Nose: Test Data\]**

```{r}
pred_RNfitTest %>% 
  roc_curve(truth= Nausea, .pred_No) %>%
  autoplot()
```

**Va. ROC Curve Performance \[Runny Nose: Test Data\]**

```{r}
pred_RNfitTest %>% #Let's check the performance <0.5= not useful, ~0.7= useful, 1= perfect
  roc_auc(truth= Nausea, .pred_No) #Sitting at 0.52 ROC-AUC is not useful
```

# This section added by SARA BENIST

Now, we will be fitting models and predicting `BodyTemp` from all symptoms.

## Create recipe with all symptoms

Following the same steps as above:

```{r}
#create recipe using all symptoms as predictors of body temp
flu_recBTAS <- 
  recipe(BodyTemp ~ ., data = train_data_flu)

#set model
ln_mod <- linear_reg() %>% 
  set_engine("glm")

#create work flow
flu_wflowBTAS <-
  workflow() %>% 
  add_model(ln_mod) %>% 
  add_recipe(flu_recBTAS)

#create fitted model
flu_fitBTAS <-
  flu_wflowBTAS %>% 
  fit(data = train_data_flu)

#check fitted model
flu_fitBTAS %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

Here, we can see the fitted model predicts Body Temperature from all symptoms with most predictors not being statistically significant. Estimates cannot be directly compared without standardizing the variables.

## Predictions from trained model

We can also make predictions using the `flu_fitBTAS` model and the `test_data_flu`.

```{r}
#create predictions
flu_augBTAS <- augment(flu_fitBTAS, test_data_flu)

#check RMSE as metric for model performance
flu_augBTAS %>% 
  rmse(truth = BodyTemp, estimate = .pred)

```

The root mean square error has an estimate of 1.230, indicating this would not be a good model for the data.

We can also use the `train_data_flu` data to make predictions.

```{r}
#predict from training data
flu_augRN2 <- augment(flu_fitBTAS, train_data_flu)

#generate RMSE for model performance
flu_augRN2 %>% 
  rmse(truth = BodyTemp, estimate = .pred)
```

The RMSE is lower for the train data, but still not an ideal value.

## Create recipe with Runny Nose

Follow the same steps with RunnyNose as the predictor.

```{r}
#create recipe using RunnyNose as predictor of body temp
flu_recBTRN <- 
  recipe(BodyTemp ~ RunnyNose, data = train_data_flu)

#set model
ln_mod <- linear_reg() %>% 
  set_engine("glm")

#create work flow
flu_wflowBTRN <-
  workflow() %>% 
  add_model(ln_mod) %>% 
  add_recipe(flu_recBTRN)

#create fitted model
flu_fitBTRN <-
  flu_wflowBTRN %>% 
  fit(data = train_data_flu)

#check fitted model
flu_fitBTRN %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

Here, the model predicts Body Temperature from Runny Nose. Having a runny nose appears to predict a lower body temperature by 0.246 degrees.

## Predictions from trained model

```{r}
#create predictions
flu_augBTRN <- augment(flu_fitBTRN, test_data_flu)

#check RMSE as metric for model performance
flu_augBTRN %>% 
  rmse(truth = BodyTemp, estimate = .pred)

```

The RMSE is similar to the all symptoms model with an estimate of 1.299.

```{r}
#predict from training data
flu_augRN3 <- augment(flu_fitBTRN, train_data_flu)

#generate RMSE for model performance
flu_augRN3 %>% 
  rmse(truth = BodyTemp, estimate = .pred)
```

Using the `train_data_flu` dataset to predict, the RMSE is lower at 1.149. None of these models appear to be productive at predicting body temperature.
